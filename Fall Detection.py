# -*- coding: utf-8 -*-
"""Narges' Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NtKEoTtam2yNNxIpkIYVDUdQp2vQ79fh
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

#from google.colab import drive
#drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/features/

"""Reset All Variable Value"""

# Commented out IPython magic to ensure Python compatibility.
#setup
#reset variavle value
from IPython import get_ipython
get_ipython().magic('reset -sf')
#intellisence
# %config IPCcompleter.greedy = True  
import numpy as np
np.random.seed(400)  # for reproducibility

"""Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import cv2
#import math
import os
import time
import numpy as np
from numpy import asarray
from numpy import clip
import random

from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image

from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions

from tensorflow.keras.applications.vgg19 import VGG19
from tensorflow.keras.applications.vgg19 import preprocess_input

import tensorflow as tf
import keras
from keras import models
from keras.models import Model
from keras import optimizers
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
from tensorflow.keras import applications
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.preprocessing.image import load_img, ImageDataGenerator, img_to_array
from keras.layers import Input
from tensorflow.keras.utils import to_categorical
from keras.utils.vis_utils import model_to_dot
from keras.layers import BatchNormalization
from keras import regularizers
from keras.regularizers import l1
from keras.regularizers import l2
from IPython.display import SVG


import matplotlib.pyplot as plt
from PIL import Image
from PIL import Image, ImageEnhance
# %matplotlib inline

#import pandas as pd
from sklearn.svm import SVC
from sklearn import svm
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score


from keras.models import Sequential
from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D,AveragePooling1D
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.datasets import load_iris
from numpy import unique

"""Pre-Process"""

def image_normalization(image):
    pixels = asarray(image)
    # convert from integers to floats
    pixels = pixels.astype('float32')
    # normalize to the range 0-1
    pixels /= 255.0
    return pixels

def image_data_gen(image):
  datagen =ImageDataGenerator(
              rotation_range=20,
              shear_range=0.2,
              width_shift_range=0.2,
              height_shift_range=0.2,
              horizontal_flip=True)
  data_generator = datagen.flow(image,batch_size=1)
  return data_generator

def image_standardization(image):
  pixels = asarray(img)
  # convert from integers to floats
  pixels = pixels.astype('float32')
  # calculate global mean and standard deviation
  mean, std = pixels.mean(), pixels.std()
  # global standardization of pixels
  pixels = (pixels - mean) / std
  # clip pixel values to [-1,1]
  pixels = clip(pixels, -1.0, 1.0)
  # shift from [-1,1] to [0,1] with 0.5 mean
  pixels = (pixels + 1.0) / 2.0
  # confirm it had the desired effect
  mean, std = pixels.mean(), pixels.std()
  return pixels

"""Pre-Train Model"""

def choose_model_4096(modell):
  input_tensor = Input(shape=(224, 224, 3))
  if(modell == 'vgg19'):
    model = applications.VGG19(weights='imagenet',include_top=True,input_tensor=input_tensor)
    custom_model = Model(model.inputs, model.layers[-3].output)
    #custom_model.summary()

  elif(modell == 'vgg16'):
    model = applications.VGG16(weights='imagenet',include_top=True,input_tensor=input_tensor,pooling=max)
    custom_model = Model(model.inputs, model.layers[-2].output)
    #custom_model.summary()

  elif(modell == 'resnet50'):
    model = applications.ResNet50(weights='imagenet', include_top=True,pooling=max)
    custom_model = Model(model.inputs, model.layers[-2].output)

  return custom_model

def choose_model_512(model_name):
  input_tensor = Input(shape=(224, 224, 3))
  if(model_name == 'vgg16'):
      custom_model = applications.VGG16(weights='imagenet',include_top=False,input_tensor=input_tensor,pooling='avg')
      custom_model.summary()

  elif(model_name == 'vgg19'):
      custom_model = applications.VGG19(weights='imagenet',include_top=False,input_tensor=input_tensor,pooling='avg')
      custom_model.summary()

  elif(model_name == 'resnet50'):
      custom_model = ResNet50(weights='imagenet', include_top=False,input_tensor=input_tensor,pooling='avg')
      custom_model.summary()

  return custom_model

"""Feature Extraction"""

#executed in 1204.502 s
def feature_extractor(frame,custom_model):
    img_vid =np.array(frame)
    img_vid = preprocess_input(img_vid)
    features = custom_model.predict(img_vid) 
    return features

def extract_fetures(path_frame,lab,model,dataset_name,class_name,custom_model,tipe,number_of_segment):   
    #keras.backend.clear_session()
    video_counter=0
    feature_list=[]
    video_number=0
    videos = sorted([vfile for vfile in os.listdir(path_frame)])
    for video in videos:
        #keras.backend.clear_session()
      # DEFINE VARIBALES
        segment_features=[]
        temp=[]
        features=[]
        img_vid = []
        frame_counter=0
        segment_counter=0
        FramInSeg=0
        
      #==================================
        video_path=os.path.join(path_frame,video)
        cap = cv2.VideoCapture(video_path)   # capturing the video from the given path
        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        segment= length//number_of_segment
      #==============================================
        print("------------------------------------------")
        extra_frame = length%number_of_segment 
        print("extra_frame",extra_frame)
        video_counter+=1
        print(video_counter)
        print(video)
        print("number of frame in each segment",segment)
        print( "video length",length )
      #+++++++++++++++++++++++++++++++++++++
        if extra_frame > 0 :
          number_of_frames = segment+1
          extra_frame -= 1
        else:
          number_of_frames = segment

        while(cap.isOpened()):
            ret, frame = cap.read()
            if (ret != True):
              print ("feature shape ",np.shape(features)) 
              #labeling
              temp.append(features)
              temp.append(lab)
              #===============
              feature_list.append(temp)  
              features=[] 
              video_number += 1
              print ("number of vido= ",video_number) 
              break        
      #+++++++++++++++++++++++++++++++++++
            frame = cv2.resize(frame, (224,224))
            #if FramInSeg <= number_of_frames :
            img_vid.append(frame)     
            frame_counter+=1
            FramInSeg+=1
            print("nubmer of frame:",frame_counter)
            if FramInSeg==number_of_frames:
              FramInSeg=0
              segment_features=feature_extractor(img_vid, custom_model)
              segment_counter+=1                 
              img_vid=[]
              print ("segment_features shape ",np.shape(segment_features)) 
              features.append(segment_features)
              segment_features=[]
              # print ("feature shape ",np.shape(features)) 
              if extra_frame > 0 :
                number_of_frames = segment+1
                extra_frame -= 1
              else:
                number_of_frames = segment                          
        cap.release()

    
    filename = tipe +'_'+ dataset_name +"_"+ class_name+'_' + model
    print("------------fatures saved----------------")
    np.save(filename, feature_list)    
    return feature_list

"""Load Feature Matrix or Extract Feature"""

#using extract featurs or use features matrix
def load_featurs(dataset_name,violent_address,nonviolent_address,model,custom_model,tipe,number_of_segment):
      
      lab=0
      VDclass_name='violent_feature'
      if os.path.isfile(tipe+'_'+dataset_name+'_'+VDclass_name+'_'+model+'.npy'):
          print ("File exist")
          violent_feature = np.load(tipe+'_'+dataset_name+'_'+VDclass_name+'_'+model+'.npy', allow_pickle=True)
      else:
          print ("File not exist")
          v_path_frame = violent_address
          violent_feature = extract_fetures(v_path_frame, lab,model,dataset_name,VDclass_name,custom_model,tipe,number_of_segment)


      label=1
      nonVDclass_name='nonviolent_feature'
      if os.path.isfile(tipe+'_'+dataset_name+'_'+nonVDclass_name+'_'+model+'.npy'):
          print ("File exist")
          nonviolent_feature= np.load(tipe+'_'+dataset_name+'_'+nonVDclass_name+'_'+model+'.npy', allow_pickle=True)
      else:
          print ("File not exist")
          nv_path_frame = nonviolent_address
          nonviolent_feature= extract_fetures(nv_path_frame, label,model,dataset_name,nonVDclass_name,custom_model,tipe,number_of_segment)
          
      print("---------------featurs shape-------------")
      print("violent_feature=",np.shape(violent_feature))
      print("nonviolent_feature=",np.shape(nonviolent_feature))
      print("-------------------------------"*3)

      return violent_feature,nonviolent_feature

"""Dataset & Split"""

def choose_dataset(dataset_name):
  dataset_name == "/Users/mahtadarvish/Desktop/Fall Detection/Data/"
  v_path_frame = "/Users/mahtadarvish/Desktop/Fall Detection/Data/Fall Sequences/"
  nv_path_frame = "/Users/mahtadarvish/Desktop/Fall Detection/Data/ADL Sequences/"
  return v_path_frame,nv_path_frame

#slpit data to train and test of each class(to have same size of each class in train and test split eavh class Separately )
def split_data(features,train_size,val_size):
  t_size=1-train_size
  train_part, testpart = train_test_split(features, train_size=train_size, test_size = t_size, random_state=1, shuffle= True)
  trainpart, valpart = train_test_split(train_part, test_size=val_size, random_state=1) # 0.25 x 0.8 = 0.2 , 0.111*0.9 = 0.1
  return trainpart,testpart,valpart

#shuffle data then seprate lable from data
def separate_label(train):
  np.random.shuffle(train)
  x_features=[]
  y_features=[]
  for i in train:
    x_features.append(i[0])
    y_features.append(i[1])
  
  return x_features,y_features

"""PMF"""

#pooling 
def pooling_feature(features,pooling_type):
  featurs_x=[]
  if pooling_type=="mean":
    for i in features:
      x=np.mean(i, axis=0)
      featurs_x.append(x)
  elif pooling_type == "max":
    for i in features:
      x=np.max(i, axis=0)
      featurs_x.append(x)
  featurs_x=np.array(featurs_x)
  return featurs_x

#pooling 
def choose_feature(features,pooling_type):
  print("------pooling----")
  featurs_x=[]
  featurs=[]
  for j in features:
      for i in j:
        x=random.choice(i)
        featurs.append(x)
      featurs_x.append(featurs)
      featurs=[]

  print(np.shape(featurs_x))
  return featurs_x

"""Classification"""

#neural network classification has 8 layer fully connected
def neural_network_classification(X_train,y_train,X_val,y_val,modell,input ):


  y_train=to_categorical(y_train)
  y_val=to_categorical(y_val)
  # Build neural network
  
  model = models.Sequential()
  model.add(Dense(4096, activation='relu',input_shape=(input,)))#layer 1
  model.add(BatchNormalization())
  model.add(Dense(2048, activation='relu'))#layer 2
  #model.add(BatchNormalization())
  model.add(Dropout(0.2))
  model.add(Dense(1024, activation='relu'))#layer 3
  model.add(Dropout(0.2))

  model.add(Dense(512, activation='relu'))#layer 4
  model.add(Dropout(0.2))

  model.add(Dense(256, activation='relu'))#layer 5
  model.add(Dropout(0.2))

  model.add(Dense(128, activation='relu'))#layer 6
  model.add(Dropout(0.2))

  model.add(Dense(2, activation='softmax'))#layer 7
  #model.summary()

  
  callback = keras.callbacks.EarlyStopping(monitor="loss",min_delta=0,patience=5,verbose=1,mode="min",baseline=None,restore_best_weights=True)
  model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=4e-5),loss='categorical_crossentropy',metrics=['accuracy'])
#(learning_rate=4e-6)
  # Train model
  history = model.fit(X_train, y_train,epochs= 100,verbose=0,shuffle=True,validation_data= (X_val,y_val),callbacks=[callback])
  return history,model

#neural network classification has 8 layer fully connected
def one_dimensional_convolution(X_train,y_train,X_val,y_val,modell,input,number_of_segment):
  keras.backend.clear_session()
  X_train=np.array(X_train)
  X_val=np.array(X_val)
  y_train=to_categorical(y_train)
  y_val=to_categorical(y_val)

  model = Sequential()
  #model.add(BatchNormalization(axis=1))
  model.add(Conv1D(filters=16, kernel_size=5, input_shape=(input, number_of_segment), activation= 'relu',padding='same'))
  model.add(AveragePooling1D(2))
  model.add(Conv1D(filters=16, kernel_size=5, activation= 'relu',padding='same'))
  model.add(BatchNormalization(axis=1))
  model.add(AveragePooling1D(2))
  
  # model.add(Dropout(0.2))

 
  model.add(Flatten())
  model.add(Dense(100, activation='relu'))
  model.add(Dense(50,activation='relu'))
  model.add(Dropout(0.2))
  model.add(Dense(2, activation='softmax'))
  
  callback = keras.callbacks.EarlyStopping(
        monitor="loss",
        # min_delta=0,
        patience=6,
        # verbose=0,
        mode="min",
        # baseline=None,
        #restore_best_weights=False
        )
  # callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=7e-5),loss='categorical_crossentropy',metrics=['accuracy'])

  history = model.fit(X_train, y_train, epochs=100,batch_size=32,verbose=0,callbacks=[callback],validation_data=(X_val,y_val))

  return history,model

# print classification_report,confusion_matrix,plot accuracy and lost
def result(x, y,history,model):
    
    print("+++ Generating result... +++")
    
    pred = model.predict(x)
#     print('First prediction:', pred)
    #y=to_categorical(y)
    score = model.evaluate(x, y,verbose=0)
    print("-------------Neural network classification----------------")
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])
    real=[]
    prediction = []
    print(np.shape(pred))

    for p in pred:
      if p[0]>=.5:
          prediction.append(0)
      elif p[1]>=.5:
          prediction.append(1)
    for i in y:
      if i[0]>=.5:
          real.append(0)
      elif i[1]>=.5:
          real.append(1)

    print("-----------------------------"*3)
    print("---------------------Classification report-----------------------")
    print("-----------------------------"*3)
    print(classification_report(real, prediction))
    
    print("-----------------------------"*3)
    print("---------------------Confusion Matrix----------------")
    print("-----------------------------"*3)
    conf_mat = confusion_matrix(real, prediction)
    print(conf_mat)
    print("-----------------------------"*3)

    print("---------plot accuracy and loss------","\n")

    plt.figure(figsize=(12,4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'val'], loc='lower right')
    
    #plt.show()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'val'], loc='upper right')
    #plt.figure(figsize=(10,5))
    plt.show()

def trans(sec):
  x=[]
  for i in sec: 
    x.append(np.transpose(i)) 
  return x

"""Fall Detection"""

def fall_detection(modell,dataset_name,pooling_type,train_size,val_size,input,tipe,number_of_segment,trainType):

  keras.backend.clear_session()
  #------------------choose pretrain model----------------------------
  if input==4096 or input==2048 :
    custom_model=choose_model_4096(modell) #choose Which model want to use for pretrain
    
  elif input==512 :
    custom_model=choose_model_512(modell)
  else :
    print("choose input")
    return 0,0

  #-------------------choose_dataset-------------------------
  v_path_frame,nv_path_frame=choose_dataset(dataset_name)#choose which datatset want to work on it

  #------------------------extract features from pretrain model---------------------
  violent_feature,nonviolent_feature = load_featurs(dataset_name,v_path_frame,nv_path_frame,modell,custom_model,tipe,number_of_segment) #load feature's matrix or extact features

    #------------------------------- classification for several times-----------------------------
  loss=[]
  acc=[]
  tag=0 # for print neural network result for 1 time and svm classification


  for i in range (0,10):
    keras.backend.clear_session()
  #-------------------- split each class to train test and validation ------------------------
    vtrain, vtest, vval = split_data(violent_feature,train_size,val_size)
    nonvtrain, nonvtest, nonvval = split_data(nonviolent_feature,train_size,val_size)

  #---------------------- combine both class fall and nonfall-----------------------
    Xtrain= np.append(vtrain, nonvtrain,axis=0)
    Xtest= np.append(vtest, nonvtest,axis=0)
    Xvalid= np.append(vval, nonvval,axis=0)


  #-----------------------seprate data frome label-------------------------
    X_train,y_train=separate_label(Xtrain)
    X_test,y_test=separate_label(Xtest)
    X_val,y_val=separate_label(Xvalid)

   # X_train,y_train,X_test,y_test,X_val,y_val=make_train_tests_data(violent_feature,nonviolent_feature,train_size,val_size)#
#---------------------temporal poling--------------------------------------
    X_train=choose_feature(X_train,pooling_type)#pmf 
    X_test=choose_feature(X_test,pooling_type)
    X_val=choose_feature(X_val,pooling_type)
    print("pooling done")
    print("X_train=",np.shape(X_train)) 
    print("X_test=",np.shape(X_test)) 
    print("X_val=",np.shape(X_val))
    if trainType=="pooling":
      X_train=pooling_feature(X_train,pooling_type)#pmf 
      X_test=pooling_feature(X_test,pooling_type)
      X_val=pooling_feature(X_val,pooling_type)  
      start = time.process_time()  
      history,model = neural_network_classification(X_train,y_train,X_val,y_val,modell,input) 
      print("process time : ",time.process_time() - start)
    elif trainType=="conv":
      X_train= trans(X_train)
      X_test= trans(X_test)
      X_val= trans(X_val)
      start = time.process_time()  
      history,model = one_dimensional_convolution(X_train,y_train,X_val,y_val,modell,input,number_of_segment)
      print("process time : ",time.process_time() - start)

    X_test=np.array(X_test) 

    #--------------------------SVM classification---------------------- 
    if tag==0:
      # print("X_train=",np.shape(X_train)) 
      # print("X_test=",np.shape(X_test)) 
      # print("X_val=",np.shape(X_val)) 
      model.summary()
      #resu=SVM_Classification(X_train,y_train,X_test)
      #print(resu)
      tag=1
  #--------------------------predict----------------------
    y=to_categorical(y_test)
    pred = model.predict(X_test)
    score = model.evaluate(X_test, y,verbose=0)
    loss.append(score[0])
    acc.append(score[1])
    result(X_test, y,history,model)
    del X_train
    del y_train
    del X_test
    del y_test
    del X_val
    del y_val
    del history  
    del model
    

  return acc,loss

"""Main"""

#Initialization
modell ='vgg16'#model can choose between  = 'vgg16','vgg19','resnet50'
dataset_name = "Data" 
pooling_type="conv" # can choose between max , mean for pooling or conv for one dimensional convolution if u change it, change the folder
train_size=0.8 # train size : 0.8 or 0.9 or .75
val_size=0.2
input=512 # choose between  (512 for using avrage pooling on last vgg layer) or (4096 for extract feartures from VGG) or (2048 for extract feature from resnet)
tipe="res" # "app" , "diff" , "appdiff" , "res" which type of input use for feature extraction
number_of_segment=20 # number of frame in each segment default is 10 if u change it change the folder
trainType="conv"
path="/Users/mahtadarvish/Desktop/Fall Detection/features/vgg19/pooling/mean/512/"
#"/features/resnet50/res/pooling/"
#"/Users/mahtadarvish/Desktop/Fall Detection/Data/ADL Sequences/"

#try :
    #os.makedirs(path)
#except Exception as e:
        #print(e)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/features/resnet50/res/pooling/

#pwd

loss=[]
acc=[]
tag=0 # for print neural network result for 1 time and svm classification
acc,loss=fall_detection(modell,dataset_name,pooling_type,train_size,val_size,input,tipe,number_of_segment,trainType)

print("-------------------")
print("+++ mean ... +++")
acc_mean= np.mean(acc)
loss_mean=np.mean(loss)
print(acc_mean)
print(loss_mean)

print("-------------------")
print("+++ median ... +++")
median_acc = np.median(acc)
median_loss = np.median(loss)
print(median_acc)
print(median_loss)
